

@article{Day2012,
  author =        {Day, Marc and Tachibana, Shigeru and Bell, John and
                   Lijewski, Michael and Beckner, Vince and
                   Cheng, Robert K.},
  journal =       {Combust. Flame},
  month =         {jan},
  number =        {1},
  pages =         {275--290},
  title =         {{A combined computational and experimental
                   characterization of lean premixed turbulent low swirl
                   laboratory flames}},
  volume =        {159},
  year =          {2012},
  doi =           {10.1016/j.combustflame.2011.06.016},
  issn =          {00102180},
  url =           {http://linkinghub.elsevier.com/retrieve/pii/
                   S0010218011001969},
}

@article{Cook1994,
  author =        {Cook, Andrew W. and Riley, James J.},
  journal =       {Phys. Fluids},
  month =         {aug},
  number =        {8},
  pages =         {2868--2870},
  title =         {{A subgrid model for equilibrium chemistry in
                   turbulent flows}},
  volume =        {6},
  year =          {1994},
  abstract =      {A method is presented whereby the fast chemistry
                   reaction, Fuel+(r)Oxidizer →(1+r) Product, may be
                   modeled in the context of a large eddy simulation
                   (LES). The model is based on a presumed form for the
                   subgrid‐scale probability density function (PDF) of
                   a conserved scalar. The nature of the subgrid‐scale
                   statistics is discussed and it is shown that a beta
                   function representation of the subgrid‐scale PDF is
                   appropriate. Data from both laboratory experiments
                   and direct numerical simulations (DNS) are used to
                   show that the predictions of the model are very
                   accurate, given the exact values for the filtered
                   scalar and its variance. A possible model for this
                   variance is presented based on scale similarity.},
  doi =           {10.1063/1.868111},
  isbn =          {9780323078917},
  issn =          {1070-6631},
  url =           {http://aip.scitation.org/doi/10.1063/1.868111},
}

@article{Jimenez1997,
  author =        {Jim{\'{e}}nez, Javier and Li{\~{n}}{\'{a}}n, Amable and
                   Rogers, Michael M. and Higuera, Francisco J.},
  journal =       {J. Fluid Mech.},
  title =         {{A priori testing of subgrid models for chemically
                   reacting non-premixed turbulent shear flows}},
  year =          {1997},
  doi =           {10.1017/S0022112097006733},
  isbn =          {00221120 (ISSN)},
  issn =          {00221120},
}

@article{Ihme2008,
  author =        {Ihme, Matthias and Pitsch, Heinz},
  journal =       {Combust. Flame},
  month =         {oct},
  number =        {1-2},
  pages =         {70--89},
  title =         {{Prediction of extinction and reignition in
                   nonpremixed turbulent flames using a
                   flamelet/progress variable model: 1. A priori study
                   and presumed PDF closure}},
  volume =        {155},
  year =          {2008},
  doi =           {10.1016/j.combustflame.2008.04.001},
  issn =          {00102180},
  url =           {https://linkinghub.elsevier.com/retrieve/pii/
                   S0010218008000904},
}

@article{Ihme2008a,
  author =        {Ihme, Matthias and Pitsch, Heinz},
  journal =       {Combust. Flame},
  month =         {oct},
  number =        {1-2},
  pages =         {90--107},
  title =         {{Prediction of extinction and reignition in
                   nonpremixed turbulent flames using a
                   flamelet/progress variable model: 2. Application in
                   LES of Sandia flames D and E}},
  volume =        {155},
  year =          {2008},
  doi =           {10.1016/j.combustflame.2008.04.015},
  issn =          {00102180},
  url =           {https://linkinghub.elsevier.com/retrieve/pii/
                   S0010218008001223},
}

@article{Fernandez-Delgado2014,
  author =        {Fern{\'{a}}ndez-Delgado, Manuel and Cernadas, Eva and
                   Barro, Sen{\'{e}}n and Amorim, Dinani and
                   {Amorim Fern{\'{a}}ndez-Delgado}, Dinani},
  journal =       {J. Mach. Learn. Res.},
  title =         {{Do we Need Hundreds of Classifiers to Solve Real
                   World Classification Problems?}},
  year =          {2014},
  abstract =      {We evaluate 179 classifiers arising from 17 families
                   (discriminant analysis, Bayesian, neural networks,
                   support vector machines, decision trees, rule-based
                   classifiers, boosting, bagging, stacking, random
                   forests and other ensembles, generalized linear
                   models, nearest-neighbors, partial least squares and
                   principal component regression, logistic and
                   multino-mial regression, multiple adaptive regression
                   splines and other methods), implemented in Weka, R
                   (with and without the caret package), C and Matlab,
                   including all the relevant classifiers available
                   today. We use 121 data sets, which represent the
                   whole UCI data base (excluding the large-scale
                   problems) and other own real problems, in order to
                   achieve significant conclusions about the classifier
                   behavior, not dependent on the data set col-lection.
                   The classifiers most likely to be the bests are the
                   random forest (RF) versions, the best of which
                   (implemented in R and accessed via caret) achieves
                   94.1{\%} of the maximum accuracy overcoming 90{\%} in
                   the 84.3{\%} of the data sets. However, the
                   dif-ference is not statistically significant with the
                   second best, the SVM with Gaussian kernel implemented
                   in C using LibSVM, which achieves 92.3{\%} of the
                   maximum accuracy. A few models are clearly better
                   than the remaining ones: random forest, SVM with
                   Gaussian and polynomial kernels, extreme learning
                   machine with Gaussian kernel, C5.0 and avNNet (a
                   committee of multi-layer perceptrons implemented in R
                   with the caret package). The random forest is clearly
                   the best family of classifiers (3 out of 5 bests
                   classifiers are RF), followed by SVM (4 classifiers
                   in the top-10), neural networks and boosting
                   ensembles (5 and 3 members in the top-20,
                   respectively).},
  doi =           {10.1016/j.csda.2008.10.033},
  isbn =          {1532-4435},
  issn =          {1532-4435},
}

@article{Liaw2002,
  author =        {Liaw, A and Wiener, M},
  journal =       {R news},
  number =        {3},
  pages =         {18--22},
  title =         {{Classification and Regression by randomForest}},
  volume =        {2},
  year =          {2002},
  abstract =      {Recently there has been a lot of interest in
                   “ensemble learning” — methods that generate
                   many classifiers and aggregate their results. Two
                   well-known methods are boosting (see, e.g., Shapire
                   et al., 1998) and bagging Breiman (1996) of
                   classification trees. In boosting, successive trees
                   give extra weight to points incorrectly predicted by
                   earlier predictors. In the end, a weighted vote is
                   taken for prediction. In bagging, successive trees do
                   not depend on earlier trees — each is independently
                   constructed using a bootstrap sample of the data set.
                   In the end, a simple majority vote is taken for
                   prediction.},
}

@article{Cybenko1989,
  author =        {Cybenko, G.},
  journal =       {Math. Control. Signals, Syst.},
  month =         {dec},
  number =        {4},
  pages =         {303--314},
  title =         {{Approximation by superpositions of a sigmoidal
                   function}},
  volume =        {2},
  year =          {1989},
  doi =           {10.1007/BF02551274},
  issn =          {0932-4194},
  url =           {http://link.springer.com/10.1007/BF02551274},
}

@article{Hornik1991,
  author =        {Hornik, Kurt},
  journal =       {Neural Networks},
  number =        {2},
  pages =         {251--257},
  title =         {{Approximation capabilities of multilayer feedforward
                   networks}},
  volume =        {4},
  year =          {1991},
  doi =           {10.1016/0893-6080(91)90009-T},
  issn =          {08936080},
  url =           {http://linkinghub.elsevier.com/retrieve/pii/
                   089360809190009T},
}

@book{Goodfellow2016,
  author =        {Goodfellow, Ian and Bengio, Yoshua and
                   Courville, Aaron},
  publisher =     {MIT Press},
  title =         {{Deep Learning}},
  year =          {2016},
  url =           {http://www.deeplearningbook.org},
}

@article{Veynante2002,
  author =        {Veynante, Denis and Vervisch, Luc},
  journal =       {Prog. Energy Combust. Sci.},
  month =         {mar},
  number =        {3},
  pages =         {193--266},
  title =         {{Turbulent combustion modeling}},
  volume =        {28},
  year =          {2002},
  abstract =      {Numerical simulation of flames is a growing field
                   bringing important improvements to our understanding
                   of combustion. The main issues and related closures
                   of turbulent combustion modeling are reviewed.
                   Combustion problems involve strong coupling between
                   chemistry, transport and fluid dynamics. The basic
                   properties of laminar flames are first presented
                   along with the major tools developed for modeling
                   turbulent combustion. The links between the available
                   closures are illuminated from a generic description
                   of modeling tools. Then, examples of numerical models
                   for mean burning rates are discussed for premixed
                   turbulent combustion. The use of direct numerical
                   simulation (DNS) as a research instrument is
                   illustrated for turbulent transport occurring in
                   premixed combustion, gradient and counter-gradient
                   modeling of turbulent fluxes is addressed. Finally, a
                   review of the models for non-premixed turbulent
                   flames is given. {\textcopyright} 2002 Published by
                   Elsevier Science Ltd.},
  doi =           {10.1016/S0360-1285(01)00017-X},
  isbn =          {978-94-007-0411-4},
  issn =          {03601285},
  url =           {http://linkinghub.elsevier.com/retrieve/pii/
                   S036012850100017X},
}

@article{Pitsch2006a,
  author =        {Pitsch, Heinz},
  journal =       {Annu. Rev. Fluid Mech.},
  month =         {jan},
  number =        {1},
  pages =         {453--482},
  title =         {{LARGE-EDDY SIMULATION OF TURBULENT COMBUSTION}},
  volume =        {38},
  year =          {2006},
  abstract =      {Turbulent dust emission is an important mechanism to
                   be considered in dust models. For example, over a
                   heated desert surface under weak wind conditions,
                   convective turbulence can be highly developed, which
                   generates patches of enhanced shear stresses and
                   entrains dust into the atmosphere. This mechanism of
                   dust emission differs from those considered in
                   existing dust emission schemes because it does not
                   have to involve the saltation of sand-sized
                   particles. In this study, we develop a large-eddy
                   dust model, WRF-LES/D, by coupling the WRF large-eddy
                   flow model with a new dust mobilization scheme. It is
                   then applied to the simulation of turbulent dust
                   emission under various stability and wind conditions.
                   Our aim is to understand how turbulent dust emission
                   occurs and how turbulent dust fluxes depend on
                   atmospheric control parameters. We show that, due to
                   the complexity of turbulent motion and the dust
                   cohesive forces, turbulent dust emission is a
                   stochastic process which needs to be statistically
                   quantified. With the numerical results, we quantify
                   the large-eddy induced shear stresses on the surface
                   and turbulent dust emissions in terms of
                   probabilistic distributions. For a given soil type,
                   it is shown that these distributions can be described
                   in terms of a few control variables, including the
                   friction velocity, u*, and the convective scaling
                   velocity, w*. {\textcopyright} 2012 Elsevier B.V.},
  doi =           {10.1146/annurev.fluid.38.050304.092133},
  isbn =          {9789048160747},
  issn =          {0066-4189},
  url =           {http://www.annualreviews.org/doi/10.1146/
                   annurev.fluid.38.050304.092133},
}

@phdthesis{VanOijen2002,
  address =       {Eindhoven},
  author =        {van Oijen, J.A},
  school =        {Eindhoven University of Technology},
  title =         {{Flamelet-Generated Manifolds: Development and
                   Application to Premixed Laminar Flames}},
  year =          {2002},
}

@article{Gicquel2000,
  author =        {Gicquel, Olivier and Darabiha, Nasser and
                   Th{\'{e}}venin, Dominique},
  journal =       {Proc. Combust. Inst.},
  month =         {jan},
  number =        {2},
  pages =         {1901--1908},
  title =         {{Liminar premixed hydrogen/air counterflow flame
                   simulations using flame prolongation of ILDM with
                   differential diffusion}},
  volume =        {28},
  year =          {2000},
  doi =           {10.1016/S0082-0784(00)80594-9},
  issn =          {15407489},
  url =           {http://linkinghub.elsevier.com/retrieve/pii/
                   S0082078400805949},
}

@article{Klimenko1999,
  author =        {Klimenko, A. Y. and Bilger, R. W.},
  journal =       {Prog. Energy Combust. Sci.},
  title =         {{Conditional moment closure for turbulent
                   combustion}},
  year =          {1999},
  abstract =      {This paper reviews the fundamentals of conditional
                   moment closure (CMC) methods for the prediction of
                   turbulent reacting flows, with particular emphasis on
                   combustion. It also surveys several of the
                   applications that have been made. CMC methods
                   predicts the conditional averages and higher moments
                   of quantities such as species mass fractions and
                   enth\ alpy, conditional on the mixture fraction or
                   reaction progress variable having a particular value.
                   A brief introduction is given to generalized
                   functions and probability density function (pdf)
                   methods. This is followed by an exposition on the
                   various methods of derivation for the CMC equation
                   and the general characteristics of this equation and
                   its boundary con\ ditions. Simplifications that can
                   be made in slender layer flows such as jets and
                   plumes are outlined and examples of application of
                   the technique to such flows are given. The method
                   allows the definition of a new class of simplified
                   reactors related to the well known perfectly stirred
                   reactor and plug flow reactor: these are outlined.
                   CMC predictions are compa\ red to experiment and
                   direct numerical simulations for flows with
                   homogeneous turbulence. Derivation and modeling of
                   the equations for conditional variances and
                   covariances are outlined and their use in
                   second-order CMC illustrated. Brief review is made of
                   progress on application of the method to problems
                   involving differential diffusion, multiple
                   conditioning,\ sprays and premixed combustion.},
  doi =           {10.1016/S0360-1285(99)00006-4},
  isbn =          {0360-1285},
  issn =          {03601285},
}

@article{JinGB08,
  author =        {Jin, B. and Grout, R. and Bushe, W. K.},
  journal =       {Flow, Turbulence and Combustion},
  month =         {Dec},
  number =        {4},
  pages =         {563--582},
  title =         {Conditional Source-Term Estimation as a Method for
                   Chemical Closure in Premixed Turbulent Reacting Flow},
  volume =        {81},
  year =          {2008},
  abstract =      {A Conditional Source-term Estimation (CSE) model is
                   used to close the mean reaction rates for a turbulent
                   premixed flame. A product-based reaction progress
                   variable is introduced as the conditioning variable
                   for the CSE method. Different presumed probability
                   density function (PDF) models are studied and a
                   modified version of a laminar flame-based PDF model
                   is proposed. Improved predictions of the variable
                   distribution are obtained. The conditional means of
                   reactive scalars are evaluated with CSE and compared
                   to the direct numerical simulation (DNS). The mean
                   reaction rates in a turbulent premixed flame are
                   evaluated with the CSE model and the presumed PDFs.
                   Comparison of the CSE closure method to DNS shows
                   promising results.},
  doi =           {10.1007/s10494-008-9148-0},
  issn =          {1573-1987},
  url =           {https://doi.org/10.1007/s10494-008-9148-0},
}

@article{Cheng2000,
  author =        {Cheng, R.K. and Yegian, D.T. and Miyasato, M.M. and
                   Samuelsen, G.S. and Benson, C.E. and Pellizzari, R and
                   Loftus, P},
  journal =       {Proc. Combust. Inst.},
  title =         {{Scaling and development of low-swirl burners for
                   low-emission furnaces and boilers}},
  year =          {2000},
  abstract =      {A low-swirl burner (LSB) developed for laboratory
                   research has been scaled to the thermal input levels
                   of a small industrial burner. The purpose was to
                   demonstrate its viability for commercial and
                   industrial furnaces and boilers. The original 5.28 cm
                   i.d. LSB using an air-jet swirler was scaled to 10.26
                   cm i.d. and investigated up to a firing rate of Q=586
                   kW. The experiments were performed in water heater
                   and furnace simulators. Subsequently, two LSBs (5.28
                   and 7.68 cm i.d.) configured to accept a novel
                   vaneswirler design were evaluated up to Q=73 kW and
                   280 kW, respectively. The larger vane-LSB was/studied
                   in a boiler simulator. The results show that a
                   constant velocity criterion is valid for scaling the
                   burner diameter to accept higher thermal inputs.
                   However, the swirl number needed for stable operation
                   should be scaled independently using a constant
                   residence time criterion. NOx emissions from all the
                   LSBs were found to be independent of thermal input
                   and were only a function of the equivalence ratio.
                   However, emissions of CO and unburned hydrocarbons
                   were strongly coupled to the combustion chamber size
                   and can be extremely high at low thermal inputs. The
                   emissions from a large vane-LSB were very
                   encouraging. Between 210 and 280 kW and
                   0.8{\textless}{\textless}0.9, NOx emissions of
                   {\textless}15 ppm and CO emissions of {\textless}10
                   ppm were achieved. These results indicate that the
                   LSB is a simple, low-cost, and promising
                   environmental energy technology that can be further
                   developed to meet future air-quality rules.},
  doi =           {10.1016/S0082-0784(00)80344-6},
  issn =          {15407489},
}

@article{Day2000,
  author =        {Day, M S and Bell, J B},
  journal =       {Combust. Theory Model.},
  month =         {dec},
  number =        {4},
  pages =         {535--556},
  title =         {{Numerical simulation of laminar reacting flows with
                   complex chemistry}},
  volume =        {4},
  year =          {2000},
  abstract =      {We present an adaptive algorithm for low Mach number
                   reacting flows with$\backslash$ncomplex chemistry.
                   Our approach uses a form of the low Mach
                   number$\backslash$nequations that discretely
                   conserves both mass and energy.
                   The$\backslash$ndiscretization methodology is based
                   on a robust projection formulation$\backslash$nthat
                   accommodates large density contrasts. The algorithm
                   uses an$\backslash$noperator-split treatment of stiff
                   reaction terms and includes effects
                   of$\backslash$ndifferential diffusion. The basic
                   computational approach is embedded in$\backslash$nan
                   adaptive projection framework that uses structured
                   hierarchical grids$\backslash$nwith subcycling in
                   time that preserves the discrete
                   conservation$\backslash$nproperties of the underlying
                   single-grid algorithm. We present
                   numerical$\backslash$nexamples illustrating the
                   performance of the method on both premixed
                   and$\backslash$nnon-premixed dames.},
  doi =           {10.1088/1364-7830/4/4/309},
  isbn =          {1364-7830$\backslash$r1741-3559},
  issn =          {1364-7830},
  url =           {http://www.tandfonline.com/doi/abs/10.1088/1364-7830/4/4/
                   309},
}

@misc{Kazakov1994,
  author =        {Kazakov, Andrei and Frenklach, Michael},
  booktitle =     {Univ. Calif. Berkeley, Berkeley, CA},
  title =         {{Reduced Reaction Sets based on GRI-Mech 1.2}},
  year =          {1994},
  url =           {http://www.me.berkeley.edu/drm/},
}

@article{Pedregosa2011,
  author =        {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and
                   Michel, V. and Thirion, B. and Grisel, O. and
                   Blondel, M. and Prettenhofer, P. and Weiss, R. and
                   Dubourg, V. and Vanderplas, J. and Passos, A. and
                   Cournapeau, D. and Brucher, M. and Perrot, M. and
                   Duchesnay, E.},
  journal =       {J. Mach. Learn. Res.},
  title =         {{Scikit-learn: Machine Learning in Python}},
  year =          {2011},
  abstract =      {Cet ouvrage retranscrit cinq entretiens,
                   diffus{\'{e}}s en 1988 sur France Culture entre
                   l'historien Roger Chartier et le sociologue Pierre
                   Bourdieu dont la pens{\'{e}}e, moins largement
                   diffus{\'{e}}e qu'aujourd'hui, {\'{e}}tait
                   d{\'{e}}j{\`{a}} l'objet de r{\'{e}}actions hostiles
                   ou d'interpr{\'{e}}tations r{\'{e}}ductrices. Chaque
                   entretien permit {\`{a}} Bourdieu de d{\'{e}}velopper
                   en direction d'un public {\'{e}}largi ses
                   r{\'{e}}flexions sur des th{\`{e}}mes comme le
                   m{\'{e}}tier de sociologue, illusions et
                   connaissance, structure et individu{\ldots} Sans que
                   soient mises en cause...},
  doi =           {10.1007/s13398-014-0173-7.2},
  isbn =          {9781783281930},
  issn =          {1271-6669},
}

@article{Breiman2001,
  author =        {Breiman, Leo},
  journal =       {Mach. Learn.},
  title =         {{Random forests}},
  year =          {2001},
  abstract =      {Random forests are a combination of tree predictors
                   such that each tree depends on the values of a random
                   vector sampled independently and with the same
                   distribution for all trees in the forest. The
                   generalization error for forests converges a.s. to a
                   limit as the number of trees in the forest becomes
                   large. The generalization error of a forest of tree
                   classifiers depends on the strength of the individual
                   trees in the forest and the correlation between them.
                   Using a random selection of features to split each
                   node yields error rates that compare favorably to
                   Adaboost (Y. Freund {\{}{\&}{\}} R. Schapire, Machine
                   Learning: Proceedings of the Thirteenth International
                   conference, ***, 148--156), but are more robust with
                   respect to noise. Internal estimates monitor error,
                   strength, and correlation and these are used to show
                   the response to increasing the number of features
                   used in the splitting. Internal estimates are also
                   used to measure variable importance. These ideas are
                   also applicable to regression.},
  doi =           {10.1023/A:1010933404324},
  isbn =          {0885-6125},
  issn =          {08856125},
}

@article{Goodfellow2014,
  author =        {Goodfellow, Ian and Pouget-Abadie, Jean and
                   Mirza, Mehdi and Xu, Bing and Warde-Farley, David and
                   Ozair, Sherjil and Courville, Aaron and
                   Bengio, Yoshua},
  journal =       {Adv. Neural Inf. Process. Syst. 27},
  pages =         {2672--2680},
  title =         {{Generative Adversarial Nets}},
  year =          {2014},
  abstract =      {We propose a new framework for estimating generative
                   models via an adversar- ial process; in which we
                   simultaneously train two models: a generative model G
                   that captures the data distribution; and a
                   discriminative model D that estimates the probability
                   that a sample came from the training data rather
                   thanG. The train- ing procedure for G is to maximize
                   the probability of D making a mistake. This framework
                   corresponds to a minimax two-player game. In the
                   space of arbitrary functions G and D; a unique
                   solution exists; with G recovering the training data
                   distribution andD equal to 1 2 everywhere. In the
                   case where G andD are defined by multilayer
                   perceptrons; the entire system can be trained with
                   backpropagation. There is no need for any Markov
                   chains or unrolled approximate inference net- works
                   during either training or generation of samples.
                   Experiments demonstrate the potential of the
                   framework through qualitative and quantitative
                   evaluation of the generated samples. 1},
  doi =           {10.1017/CBO9781139058452},
  isbn =          {1406.2661},
  issn =          {10495258},
  url =           {http://papers.nips.cc/paper/5423-generative-adversarial-
                   nets.pdf},
}

@inproceedings{Burger2012,
  author =        {Burger, Harold C. and Schuler, Christian J. and
                   Harmeling, Stefan},
  booktitle =     {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern
                   Recognit.},
  pages =         {2392--2399},
  title =         {{Image denoising: Can plain neural networks compete
                   with BM3D?}},
  year =          {2012},
  abstract =      {Image denoising can be described as the problem of
                   mapping from a noisy image to a noise-free image. The
                   best currently available denoising methods
                   approximate this mapping with cleverly engineered
                   algorithms. In this work we attempt to learn this
                   mapping directly with a plain multi layer perceptron
                   (MLP) applied to image patches. While this has been
                   done before, we will show that by training on large
                   image databases we are able to compete with the
                   current state-of-the-art image denoising methods.
                   Furthermore, our approach is easily adapted to less
                   extensively studied types of noise (by merely
                   exchanging the training data), for which we achieve
                   excellent results as well.},
  doi =           {10.1109/CVPR.2012.6247952},
  isbn =          {9781467312264},
  issn =          {10636919},
}

@inproceedings{Dosovitskiy2015,
  author =        {Dosovitskiy, Alexey and Springenberg, Jost Tobias and
                   Brox, Thomas},
  booktitle =     {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern
                   Recognit.},
  pages =         {1538--1546},
  title =         {{Learning to generate chairs with convolutional
                   neural networks}},
  volume =        {07-12-June},
  year =          {2015},
  abstract =      {There are two widely known issues with prop- erly
                   training Recurrent Neural Networks, the vanishing and
                   the exploding gradient prob- lems detailed in Bengio
                   et al. (1994). In this paper we attempt to improve
                   the under- standing of the underlying issues by
                   explor- ing these problems from an analytical, a geo-
                   metric and a dynamical systems perspective. Our
                   analysis is used to justify a simple yet ef- fective
                   solution. We propose a gradient norm clipping
                   strategy to deal with exploding gra- dients and a
                   soft constraint for the vanishing gradients problem.
                   We validate empirically our hypothesis and proposed
                   solutions in the experimental section.},
  doi =           {10.1109/CVPR.2015.7298761},
  isbn =          {9781467369640},
  issn =          {10636919},
}

@article{Lefkimmiatis2016,
  author =        {Lefkimmiatis, Stamatios},
  journal =       {Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  month =         {nov},
  pages =         {5882--5891},
  title =         {{Non-Local Color Image Denoising with Convolutional
                   Neural Networks}},
  year =          {2016},
  abstract =      {We propose a novel deep network architecture for
                   grayscale and color image denoising that is based on
                   a non-local image model. Our motivation for the
                   overall design of the proposed network stems from
                   variational methods that exploit the inherent
                   non-local self-similarity property of natural images.
                   We build on this concept and introduce deep networks
                   that perform non-local processing and at the same
                   time they significantly benefit from discriminative
                   learning. Experiments on the Berkeley segmentation
                   dataset, comparing several state-of-the-art methods,
                   show that the proposed non-local models achieve the
                   best reported denoising performance both for
                   grayscale and color images for all the tested noise
                   levels. It is also worth noting that this increase in
                   performance comes at no extra cost on the capacity of
                   the network compared to existing alternative deep
                   network architectures. In addition, we highlight a
                   direct link of the proposed non-local models to
                   convolutional neural networks. This connection is of
                   significant importance since it allows our models to
                   take full advantage of the latest advances on GPU
                   computing in deep learning and makes them amenable to
                   efficient implementations through their inherent
                   parallelism.},
  doi =           {10.1109/CVPR.2017.623},
  url =           {http://arxiv.org/abs/1611.06757},
}

@article{Ledig2017,
  author =        {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and
                   Caballero, Jose and Cunningham, Andrew and
                   Acosta, Alejandro and Aitken, Andrew and
                   Tejani, Alykhan and Totz, Johannes and Wang, Zehan and
                   Shi, Wenzhe},
  journal =       {Conf. Comput. Vis. Pattern Recognit.},
  month =         {sep},
  pages =         {1--14},
  title =         {{Photo-Realistic Single Image Super-Resolution Using
                   a Generative Adversarial Network}},
  year =          {2016},
  abstract =      {Despite the breakthroughs in accuracy and speed of
                   single image super-resolution using faster and deeper
                   convolutional neural networks, one central problem
                   remains largely unsolved: how do we recover the finer
                   texture details when we super-resolve at large
                   upscaling factors? The behavior of optimization-based
                   super-resolution methods is principally driven by the
                   choice of the objective function. Recent work has
                   largely focused on minimizing the mean squared
                   reconstruction error. The resulting estimates have
                   high peak signal-to-noise ratios, but they are often
                   lacking high-frequency details and are perceptually
                   unsatisfying in the sense that they fail to match the
                   fidelity expected at the higher resolution. In this
                   paper, we present SRGAN, a generative adversarial
                   network (GAN) for image super-resolution (SR). To our
                   knowledge, it is the first framework capable of
                   inferring photo-realistic natural images for 4x
                   upscaling factors. To achieve this, we propose a
                   perceptual loss function which consists of an
                   adversarial loss and a content loss. The adversarial
                   loss pushes our solution to the natural image
                   manifold using a discriminator network that is
                   trained to differentiate between the super-resolved
                   images and original photo-realistic images. In
                   addition, we use a content loss motivated by
                   perceptual similarity instead of similarity in pixel
                   space. Our deep residual network is able to recover
                   photo-realistic textures from heavily downsampled
                   images on public benchmarks. An extensive
                   mean-opinion-score (MOS) test shows hugely
                   significant gains in perceptual quality using SRGAN.
                   The MOS scores obtained with SRGAN are closer to
                   those of the original high-resolution images than to
                   those obtained with any state-of-the-art method.},
  doi =           {10.1109/CVPR.2017.19},
  isbn =          {978-1-5386-0457-1},
  issn =          {0018-5043},
  url =           {http://arxiv.org/abs/1609.04802},
}

@inproceedings{Tai2017,
  author =        {Tai, Ying and Yang, Jian and Liu, Xiaoming},
  booktitle =     {Proc. - 30th IEEE Conf. Comput. Vis. Pattern
                   Recognition, CVPR 2017},
  pages =         {2790--2798},
  title =         {{Image super-resolution via deep recursive residual
                   network}},
  volume =        {2017-Janua},
  year =          {2017},
  abstract =      {Recently, Convolutional Neural Network (CNN) based
                   models have achieved great success in Single Image
                   Super-Resolution (SISR). Owing to the strength of
                   deep networks, these CNN models learn an effective
                   nonlinear mapping from the low-resolution input image
                   to the high-resolution target image, at the cost of
                   requiring enormous parameters. This paper proposes a
                   very deep CNN model (up to 52 con-volutional layers)
                   named Deep Recursive Residual Network (DRRN) that
                   strives for deep yet concise networks. Specifi-cally,
                   residual learning is adopted, both in global and
                   local manners, to mitigate the difficulty of training
                   very deep net-works; recursive learning is used to
                   control the model pa-rameters while increasing the
                   depth. Extensive benchmark evaluation shows that DRRN
                   significantly outperforms state of the art in SISR,
                   while utilizing far fewer parameters. Code is
                   available at https://github.com/tyshiwo /DRRN
                   CVPR17.},
  doi =           {10.1109/CVPR.2017.298},
  isbn =          {9781538604571},
}

@inproceedings{Lai2017,
  author =        {Lai, Wei Sheng and Huang, Jia Bin and Ahuja, Narendra and
                   Yang, Ming Hsuan},
  booktitle =     {Proc. - 30th IEEE Conf. Comput. Vis. Pattern
                   Recognition, CVPR 2017},
  pages =         {5835--5843},
  title =         {{Deep laplacian pyramid networks for fast and
                   accurate super-resolution}},
  volume =        {2017-Janua},
  year =          {2017},
  abstract =      {Convolutional neural networks have recently
                   demonstrated high-quality reconstruction for single
                   image super-resolution. However, existing methods
                   often require a large number of network parameters
                   and entail heavy computational loads at runtime for
                   generating high-accuracy super-resolution results. In
                   this paper, we propose the deep Laplacian Pyramid
                   Super-Resolution Network for fast and accurate image
                   super-resolution. The proposed network progressively
                   reconstructs the sub-band residuals of
                   high-resolution images at multiple pyramid levels. In
                   contrast to existing methods that involve the bicubic
                   interpolation for pre-processing (which results in
                   large feature maps), the proposed method directly
                   extracts features from the low-resolution input space
                   and thereby entails low computational loads. We train
                   the proposed network with deep supervision using the
                   robust Charbonnier loss functions and achieve
                   high-quality image reconstruction. Furthermore, we
                   utilize the recursive layers to share parameters
                   across as well as within pyramid levels, and thus
                   drastically reduce the number of parameters.
                   Extensive quantitative and qualitative evaluations on
                   benchmark datasets show that the proposed algorithm
                   performs favorably against the state-of-the-art
                   methods in terms of run-time and image quality.},
  doi =           {10.1109/CVPR.2017.618},
  isbn =          {9781538604571},
  issn =          {1063-6919},
}

@article{Graves2013,
  author =        {Graves, Alex},
  journal =       {arXiv:1308.0850},
  title =         {{Generating sequences with recurrent neural networks.
                   preprint}},
  year =          {2013},
  abstract =      {This paper shows how Long Short-term Memory recurrent
                   neural networks can be used to generate complex
                   sequences with long-range structure, simply by
                   predicting one data point at a time. The approach is
                   demonstrated for text (where the data are discrete)
                   and online handwriting (where the data are
                   real-valued). It is then extended to handwriting
                   synthesis by allowing the network to condition its
                   predictions on a text sequence. The resulting system
                   is able to generate highly realistic cursive
                   handwriting in a wide variety of styles.},
  doi =           {10.1145/2661829.2661935},
  isbn =          {2000201075},
  issn =          {18792782},
}

@article{Wu2016,
  author =        {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and
                   Le, Quoc V. and Norouzi, Mohammad and
                   Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and
                   Gao, Qin and Macherey, Klaus and Klingner, Jeff and
                   Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and
                   Kaiser, {\L}ukasz and Gouws, Stephan and
                   Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and
                   Stevens, Keith and Kurian, George and Patil, Nishant and
                   Wang, Wei and Young, Cliff and Smith, Jason and
                   Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and
                   Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  journal =       {ArXiv e-prints},
  title =         {{Google's Neural Machine Translation System: Bridging
                   the Gap between Human and Machine Translation}},
  year =          {2016},
  abstract =      {Neural Machine Translation (NMT) is an end-to-end
                   learning approach for automated translation, with the
                   potential to overcome many of the weaknesses of
                   conventional phrase-based translation systems.
                   Unfortunately, NMT systems are known to be
                   computationally expensive both in training and in
                   translation inference. Also, most NMT systems have
                   difficulty with rare words. These issues have
                   hindered NMT's use in practical deployments and
                   services, where both accuracy and speed are
                   essential. In this work, we present GNMT, Google's
                   Neural Machine Translation system, which attempts to
                   address many of these issues. Our model consists of a
                   deep LSTM network with 8 encoder and 8 decoder layers
                   using attention and residual connections. To improve
                   parallelism and therefore decrease training time, our
                   attention mechanism connects the bottom layer of the
                   decoder to the top layer of the encoder. To
                   accelerate the final translation speed, we employ
                   low-precision arithmetic during inference
                   computations. To improve handling of rare words, we
                   divide words into a limited set of common sub-word
                   units ("wordpieces") for both input and output. This
                   method provides a good balance between the
                   flexibility of "character"-delimited models and the
                   efficiency of "word"-delimited models, naturally
                   handles translation of rare words, and ultimately
                   improves the overall accuracy of the system. Our beam
                   search technique employs a length-normalization
                   procedure and uses a coverage penalty, which
                   encourages generation of an output sentence that is
                   most likely to cover all the words in the source
                   sentence. On the WMT'14 English-to-French and
                   English-to-German benchmarks, GNMT achieves
                   competitive results to state-of-the-art. Using a
                   human side-by-side evaluation on a set of isolated
                   simple sentences, it reduces translation errors by an
                   average of 60{\%} compared to Google's phrase-based
                   production system.},
  doi =           {10.1038/nrn2258},
  isbn =          {1471-0048 (Electronic)$\backslash$r1471-003X
                   (Linking)},
  issn =          {1471003X},
}

@article{Kwon2017,
  author =        {Kwon, Whi},
  journal =       {arXiv1706.03762 [cs]},
  title =         {{Attention Is All You Need}},
  year =          {2017},
  abstract =      {The dominant sequence transduction models are based
                   on complex recurrent or convolutional neural networks
                   in an encoder-decoder configuration. The best
                   performing models also connect the encoder and
                   decoder through an attention mechanism. We propose a
                   new simple network architecture, the Transformer,
                   based solely on attention mechanisms, dispensing with
                   recurrence and convolutions entirely. Experiments on
                   two machine translation tasks show these models to be
                   superior in quality while being more parallelizable
                   and requiring significantly less time to train. Our
                   model achieves 28.4 BLEU on the WMT 2014
                   English-to-German translation task, improving over
                   the existing best results, including ensembles by
                   over 2 BLEU. On the WMT 2014 English-to-French
                   translation task, our model establishes a new
                   single-model state-of-the-art BLEU score of 41.0
                   after training for 3.5 days on eight GPUs, a small
                   fraction of the training costs of the best models
                   from the literature. We show that the Transformer
                   generalizes well to other tasks by applying it
                   successfully to English constituency parsing both
                   with large and limited training data.},
}

@article{Silver2017,
  author =        {Silver, David and Schrittwieser, Julian and
                   Simonyan, Karen and Antonoglou, Ioannis and
                   Huang, Aja and Guez, Arthur and Hubert, Thomas and
                   Baker, Lucas and Lai, Matthew and Bolton, Adrian and
                   Chen, Yutian and Lillicrap, Timothy and Hui, Fan and
                   Sifre, Laurent and van den Driessche, George and
                   Graepel, Thore and Hassabis, Demis},
  journal =       {Nature},
  month =         {oct},
  number =        {7676},
  pages =         {354--359},
  title =         {{Mastering the game of Go without human knowledge}},
  volume =        {550},
  year =          {2017},
  abstract =      {Starting from zero knowledge and without human data,
                   AlphaGo Zero was able to teach itself to play Go and
                   to develop novel strategies that provide new insights
                   into the oldest of games.},
  doi =           {10.1038/nature24270},
  isbn =          {3013372370},
  issn =          {0028-0836},
  url =           {http://www.nature.com/doifinder/10.1038/nature24270},
}

@article{Lecun2015,
  author =        {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal =       {Nature},
  number =        {7553},
  pages =         {436--444},
  title =         {{Deep learning}},
  volume =        {521},
  year =          {2015},
  abstract =      {Deep learning allows computational models that are
                   composed of multiple processing layers to learn
                   representations of data with multiple levels of
                   abstraction. These methods have dramatically improved
                   the state-of-the-art in speech recognition, visual
                   object recognition, object detection and many other
                   domains such as drug discovery and genomics. Deep
                   learning discovers intricate structure in large data
                   sets by using the backpropagation algorithm to
                   indicate how a machine should change its internal
                   parameters that are used to compute the
                   representation in each layer from the representation
                   in the previous layer. Deep convolutional nets have
                   brought about breakthroughs in processing images,
                   video, speech and audio, whereas recurrent nets have
                   shone light on sequential data such as text and
                   speech.},
  doi =           {10.1038/nature14539},
  isbn =          {9780521835688},
  issn =          {14764687},
}

@article{Schmidhuber2015,
  author =        {Schmidhuber, J{\"{u}}rgen},
  journal =       {Neural Networks},
  pages =         {85--117},
  publisher =     {Elsevier Ltd},
  title =         {{Deep Learning in neural networks: An overview}},
  volume =        {61},
  year =          {2015},
  abstract =      {In recent years, deep artificial neural networks
                   (including recurrent ones) have won numerous contests
                   in pattern recognition and machine learning. This
                   historical survey compactly summarizes relevant work,
                   much of it from the previous millennium. Shallow and
                   Deep Learners are distinguished by the depth of their
                   credit assignment paths, which are chains of possibly
                   learnable, causal links between actions and effects.
                   I review deep supervised learning (also
                   recapitulating the history of backpropagation),
                   unsupervised learning, reinforcement learning {\&}
                   evolutionary computation, and indirect search for
                   short programs encoding deep and large networks.},
  doi =           {10.1016/j.neunet.2014.09.003},
  isbn =          {0893-6080},
  issn =          {18792782},
  url =           {http://dx.doi.org/10.1016/j.neunet.2014.09.003},
}

@article{Prieto2016,
  author =        {Prieto, Alberto and Prieto, Beatriz and
                   Ortigosa, Eva Martinez and Ros, Eduardo and
                   Pelayo, Francisco and Ortega, Julio and
                   Rojas, Ignacio},
  journal =       {Neurocomputing},
  title =         {{Neural networks: An overview of early research,
                   current frameworks and new challenges}},
  year =          {2016},
  abstract =      {This paper presents a comprehensive overview of
                   modelling, simulation and implementation of neural
                   networks, taking into account that two aims have
                   emerged in this area: the improvement of our
                   understanding of the behaviour of the nervous system
                   and the need to find inspiration from it to build
                   systems with the advantages provided by nature to
                   perform certain relevant tasks. The development and
                   evolution of different topics related to neural
                   networks is described (simulators, implementations,
                   and real-world applications) showing that the field
                   has acquired maturity and consolidation, proven by
                   its competitiveness in solving real-world problems.
                   The paper also shows how, over time, artificial
                   neural networks have contributed to fundamental
                   concepts at the birth and development of other
                   disciplines such as Computational Neuroscience,
                   Neuro-engineering, Computational Intelligence and
                   Machine Learning. A better understanding of the human
                   brain is considered one of the challenges of this
                   century, and to achieve it, as this paper goes on to
                   describe, several important national and
                   multinational projects and initiatives are marking
                   the way to follow in neural-network research.},
  doi =           {10.1016/j.neucom.2016.06.014},
  issn =          {18728286},
}

@article{Liu2017,
  author =        {Liu, Weibo and Wang, Zidong and Liu, Xiaohui and
                   Zeng, Nianyin and Liu, Yurong and Alsaadi, Fuad E.},
  journal =       {Neurocomputing},
  title =         {{A survey of deep neural network architectures and
                   their applications}},
  year =          {2017},
  abstract =      {Since the proposal of a fast learning algorithm for
                   deep belief networks in 2006, the deep learning
                   techniques have drawn ever-increasing research
                   interests because of their inherent capability of
                   overcoming the drawback of traditional algorithms
                   dependent on hand-designed features. Deep learning
                   approaches have also been found to be suitable for
                   big data analysis with successful applications to
                   computer vision, pattern recognition, speech
                   recognition, natural language processing, and
                   recommendation systems. In this paper, we discuss
                   some widely-used deep learning architectures and
                   their practical applications. An up-to-date overview
                   is provided on four deep learning architectures,
                   namely, autoencoder, convolutional neural network,
                   deep belief network, and restricted Boltzmann
                   machine. Different types of deep neural networks are
                   surveyed and recent progresses are summarized.
                   Applications of deep learning techniques on some
                   selected areas (speech recognition, pattern
                   recognition and computer vision) are highlighted. A
                   list of future research topics are finally given with
                   clear justifications.},
  doi =           {10.1016/j.neucom.2016.12.038},
  isbn =          {0925-2312},
  issn =          {18728286},
}

@article{Ioffe2015,
  author =        {Ioffe, Sergey and Szegedy, Christian},
  month =         {feb},
  title =         {{Batch Normalization: Accelerating Deep Network
                   Training by Reducing Internal Covariate Shift}},
  year =          {2015},
  abstract =      {Training Deep Neural Networks is complicated by the
                   fact that the distribution of each layer's inputs
                   changes during training, as the parameters of the
                   previous layers change. This slows down the training
                   by requiring lower learning rates and careful
                   parameter initialization, and makes it notoriously
                   hard to train models with saturating nonlinearities.
                   We refer to this phenomenon as internal covariate
                   shift, and address the problem by normalizing layer
                   inputs. Our method draws its strength from making
                   normalization a part of the model architecture and
                   performing the normalization for each training
                   mini-batch. Batch Normalization allows us to use much
                   higher learning rates and be less careful about
                   initialization. It also acts as a regularizer, in
                   some cases eliminating the need for Dropout. Applied
                   to a state-of-the-art image classification model,
                   Batch Normalization achieves the same accuracy with
                   14 times fewer training steps, and beats the original
                   model by a significant margin. Using an ensemble of
                   batch-normalized networks, we improve upon the best
                   published result on ImageNet classification: reaching
                   4.9{\%} top-5 validation error (and 4.8{\%} test
                   error), exceeding the accuracy of human raters.},
  url =           {http://arxiv.org/abs/1502.03167},
}

@article{Kingma2014,
  author =        {Kingma, Diederik P. and Ba, Jimmy},
  month =         {dec},
  title =         {{Adam: A Method for Stochastic Optimization}},
  year =          {2014},
  abstract =      {We introduce Adam, an algorithm for first-order
                   gradient-based optimization of stochastic objective
                   functions, based on adaptive estimates of lower-order
                   moments. The method is straightforward to implement,
                   is computationally efficient, has little memory
                   requirements, is invariant to diagonal rescaling of
                   the gradients, and is well suited for problems that
                   are large in terms of data and/or parameters. The
                   method is also appropriate for non-stationary
                   objectives and problems with very noisy and/or sparse
                   gradients. The hyper-parameters have intuitive
                   interpretations and typically require little tuning.
                   Some connections to related algorithms, on which Adam
                   was inspired, are discussed. We also analyze the
                   theoretical convergence properties of the algorithm
                   and provide a regret bound on the convergence rate
                   that is comparable to the best known results under
                   the online convex optimization framework. Empirical
                   results demonstrate that Adam works well in practice
                   and compares favorably to other stochastic
                   optimization methods. Finally, we discuss AdaMax, a
                   variant of Adam based on the infinity norm.},
  url =           {http://arxiv.org/abs/1412.6980},
}

@inproceedings{Paszke2017,
  author =        {Paszke, Adam and Gross, Sam and Chintala, Soumith and
                   Chanan, Gregory and Yang, Edward and DeVito, Zachary and
                   Lin, Zeming and Desmaison, Alban and Antiga, Luca and
                   Lerer, Adam},
  booktitle =     {NIPS-W},
  title =         {{Automatic differentiation in PyTorch}},
  year =          {2017},
}

@article{Kingma2013,
  author =        {Kingma, Diederik P and Welling, Max},
  month =         {dec},
  title =         {{Auto-Encoding Variational Bayes}},
  year =          {2013},
  abstract =      {How can we perform efficient inference and learning
                   in directed probabilistic models, in the presence of
                   continuous latent variables with intractable
                   posterior distributions, and large datasets? We
                   introduce a stochastic variational inference and
                   learning algorithm that scales to large datasets and,
                   under some mild differentiability conditions, even
                   works in the intractable case. Our contributions is
                   two-fold. First, we show that a reparameterization of
                   the variational lower bound yields a lower bound
                   estimator that can be straightforwardly optimized
                   using standard stochastic gradient methods. Second,
                   we show that for i.i.d. datasets with continuous
                   latent variables per datapoint, posterior inference
                   can be made especially efficient by fitting an
                   approximate inference model (also called a
                   recognition model) to the intractable posterior using
                   the proposed lower bound estimator. Theoretical
                   advantages are reflected in experimental results.},
  url =           {http://arxiv.org/abs/1312.6114},
}

@article{Rezende2014,
  author =        {Rezende, Danilo Jimenez and Mohamed, Shakir and
                   Wierstra, Daan},
  month =         {jan},
  title =         {{Stochastic Backpropagation and Approximate Inference
                   in Deep Generative Models}},
  year =          {2014},
  abstract =      {We marry ideas from deep neural networks and
                   approximate Bayesian inference to derive a
                   generalised class of deep, directed generative
                   models, endowed with a new algorithm for scalable
                   inference and learning. Our algorithm introduces a
                   recognition model to represent approximate posterior
                   distributions, and that acts as a stochastic encoder
                   of the data. We develop stochastic back-propagation
                   -- rules for back-propagation through stochastic
                   variables -- and use this to develop an algorithm
                   that allows for joint optimisation of the parameters
                   of both the generative and recognition model. We
                   demonstrate on several real-world data sets that the
                   model generates realistic samples, provides accurate
                   imputations of missing data and is a useful tool for
                   high-dimensional data visualisation.},
  url =           {http://arxiv.org/abs/1401.4082},
}

@article{Chen2016a,
  author =        {Chen, Xi and Duan, Yan and Houthooft, Rein and
                   Schulman, John and Sutskever, Ilya and
                   Abbeel, Pieter},
  month =         {jun},
  title =         {{InfoGAN: Interpretable Representation Learning by
                   Information Maximizing Generative Adversarial Nets}},
  year =          {2016},
  abstract =      {This paper describes InfoGAN, an
                   information-theoretic extension to the Generative
                   Adversarial Network that is able to learn
                   disentangled representations in a completely
                   unsupervised manner. InfoGAN is a generative
                   adversarial network that also maximizes the mutual
                   information between a small subset of the latent
                   variables and the observation. We derive a lower
                   bound to the mutual information objective that can be
                   optimized efficiently, and show that our training
                   procedure can be interpreted as a variation of the
                   Wake-Sleep algorithm. Specifically, InfoGAN
                   successfully disentangles writing styles from digit
                   shapes on the MNIST dataset, pose from lighting of 3D
                   rendered images, and background digits from the
                   central digit on the SVHN dataset. It also discovers
                   visual concepts that include hair styles,
                   presence/absence of eyeglasses, and emotions on the
                   CelebA face dataset. Experiments show that InfoGAN
                   learns interpretable representations that are
                   competitive with representations learned by existing
                   fully supervised methods.},
  url =           {http://arxiv.org/abs/1606.03657},
}

@misc{Endres2003,
  author =        {Endres, Dominik M. and Schindelin, Johannes E.},
  booktitle =     {IEEE Trans. Inf. Theory},
  title =         {{A new metric for probability distributions}},
  year =          {2003},
  abstract =      {We introduce a metric for probability distributions,
                   which is bounded, information-theoretically
                   motivated, and has a natural Bayesian interpretation.
                   The square root of the well-known $\chi$2 distance is
                   an asymptotic approximation to it. Moreover, it is a
                   close relative of the capacitory discrimination and
                   Jensen-Shannon divergence.},
  doi =           {10.1109/TIT.2003.813506},
  isbn =          {0018-9448},
  issn =          {00189448},
}

@article{Osterreicher2003,
  author =        {{\"{O}}sterreicher, Ferdinand and Vajda, Igor},
  journal =       {Ann. Inst. Stat. Math.},
  title =         {{A new class of metric divergences on probability
                   spaces and its applicability in statistics}},
  year =          {2003},
  abstract =      {The classI f $\beta$, $\beta$$\epsilon$(0, ∞],
                   off-divergences investigated in this paper is defined
                   in terms of a class of entropies introduced by
                   Arimoto (1971,Information and Control,19, 181–194).
                   It contains the squared Hellinger distance (for
                   $\beta$=1/2), the sumI(Q 1‖(Q 1+Q 2)/2)+I(Q 2‖(Q
                   1+Q 2)/2) of Kullback-Leibler divergences (for
                   $\beta$=1) and half of the variation distance (for
                   $\beta$=∞) and continuously extends the class of
                   squared perimeter-type distances introduced by
                   {\"{O}}sterreicher (1996,Kybernetika,32, 389–393)
                   (for $\beta$$\epsilon$ (1, ∞]). It is shown that
                   (If$\beta$(Q1,Q2))min($\beta$,1/2) are distances of
                   probability distributionsQ 1,Q 2 for $\beta$
                   $\epsilon$ (0, ∞). The applicability of If$\beta$
                   -divergences in statistics is also considered. In
                   particular, it is shown that the If$\beta$
                   -projections of appropriate empirical distributions
                   to regular families define distribution estimates
                   which are in the case of an i.i.d. sample of size'n
                   consistent. The order of consistency is investigated
                   as well.},
  doi =           {10.1007/BF02517812},
  isbn =          {0743-7463},
  issn =          {00203157},
}

@article{Kullback1987,
  author =        {Kullback, S.},
  journal =       {Am. Stat.},
  month =         {nov},
  number =        {4},
  pages =         {338--341},
  title =         {{Letters to the Editor}},
  volume =        {41},
  year =          {1987},
  doi =           {10.1080/00031305.1987.10475510},
  issn =          {0003-1305},
  url =           {http://www.tandfonline.com/doi/abs/10.1080/
                   00031305.1987.10475510},
}

@misc{Jones2001,
  author =        {Jones, Eric and Oliphant, Travis and Peterson, Pearu and
                   others},
  note =          {[Online; accessed 12/10/2018]},
  title =         {{SciPy}: Open source scientific tools for {Python}},
  year =          {2001--},
  url =           {http://www.scipy.org/},
}

